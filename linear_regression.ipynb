{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML Logo](http://spark-mooc.github.io/web-assets/images/CS190.1x_Banner_300.png)\n",
    "# **Linear Regression Lab**\n",
    "This lab covers a common supervised learning pipeline, using a subset of the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD). Our goal is to train a linear regression model to predict the release year of a song given a set of audio features.\n",
    "\n",
    "The first part is based on building the linear model using Sklearn with a single process. The second part is essentially from the online course [BerkeleyX: CS190.1x Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/info), in which we will use Spark and MLlib with two processes on a virtual machine.\n",
    "\n",
    "###  This lab will cover: \n",
    "+   *Part 1: Using Sklearn on a single process*\n",
    " +  *Sec. 1.1:* Separating training, validation and test datasets\n",
    " +  *Sec. 1.2:* Linear Regression model training\n",
    " +  *Sec. 1.3:* Grid Search \n",
    " +  *Sec. 1.4:* Polynomial regression\n",
    "+   *Part 2: Using Spark and MLlib*\n",
    " +  *Sec. 2.1:* Read and parse the initial dataset\n",
    " +  *Sec. 2.2:* Baseline model\n",
    " +  *Sec. 2.3:* Train (via gradient descent) and evaluate a linear regression model\n",
    " +  *Sec. 2.4:* Train using MLlib and tune hyperparameters via grid search\n",
    " +  *Sec. 2.5:* Add interactions between features \n",
    "\n",
    "By comparison, we can conclude that the second-order polynomial linear model (with $\\scriptsize x^2_1, x_1x_2, \\cdots $ etc) has better prediciton performance than the linear model only with linear predictors ($\\scriptsize x_1, x_2, \\cdots $).\n",
    "\n",
    "Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labVersion = 'cs190_week3_v_1_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.884124</td>\n",
       "      <td>0.610454</td>\n",
       "      <td>0.600498</td>\n",
       "      <td>0.474669</td>\n",
       "      <td>0.247233</td>\n",
       "      <td>0.357306</td>\n",
       "      <td>0.344136</td>\n",
       "      <td>0.339641</td>\n",
       "      <td>0.600859</td>\n",
       "      <td>0.425705</td>\n",
       "      <td>0.604915</td>\n",
       "      <td>0.419193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.854412</td>\n",
       "      <td>0.604125</td>\n",
       "      <td>0.593634</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.266308</td>\n",
       "      <td>0.261472</td>\n",
       "      <td>0.506387</td>\n",
       "      <td>0.464454</td>\n",
       "      <td>0.665799</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>0.445219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.908983</td>\n",
       "      <td>0.632063</td>\n",
       "      <td>0.557429</td>\n",
       "      <td>0.498264</td>\n",
       "      <td>0.276396</td>\n",
       "      <td>0.312810</td>\n",
       "      <td>0.448530</td>\n",
       "      <td>0.448674</td>\n",
       "      <td>0.649791</td>\n",
       "      <td>0.489869</td>\n",
       "      <td>0.591908</td>\n",
       "      <td>0.450002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.842525</td>\n",
       "      <td>0.561827</td>\n",
       "      <td>0.508715</td>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.296734</td>\n",
       "      <td>0.250214</td>\n",
       "      <td>0.488541</td>\n",
       "      <td>0.360509</td>\n",
       "      <td>0.575435</td>\n",
       "      <td>0.361006</td>\n",
       "      <td>0.678379</td>\n",
       "      <td>0.409037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>0.909303</td>\n",
       "      <td>0.653608</td>\n",
       "      <td>0.585581</td>\n",
       "      <td>0.473251</td>\n",
       "      <td>0.251417</td>\n",
       "      <td>0.326977</td>\n",
       "      <td>0.404323</td>\n",
       "      <td>0.371155</td>\n",
       "      <td>0.629402</td>\n",
       "      <td>0.482243</td>\n",
       "      <td>0.566901</td>\n",
       "      <td>0.463374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4         5         6         7   \\\n",
       "0  2001  0.884124  0.610454  0.600498  0.474669  0.247233  0.357306  0.344136   \n",
       "1  2001  0.854412  0.604125  0.593634  0.495885  0.266308  0.261472  0.506387   \n",
       "2  2001  0.908983  0.632063  0.557429  0.498264  0.276396  0.312810  0.448530   \n",
       "3  2001  0.842525  0.561827  0.508715  0.443531  0.296734  0.250214  0.488541   \n",
       "4  2001  0.909303  0.653608  0.585581  0.473251  0.251417  0.326977  0.404323   \n",
       "\n",
       "         8         9         10        11        12  \n",
       "0  0.339641  0.600859  0.425705  0.604915  0.419193  \n",
       "1  0.464454  0.665799  0.542969  0.580444  0.445219  \n",
       "2  0.448674  0.649791  0.489869  0.591908  0.450002  \n",
       "3  0.360509  0.575435  0.361006  0.678379  0.409037  \n",
       "4  0.371155  0.629402  0.482243  0.566901  0.463374  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('millionsong.txt', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6724, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 1: Using sklearn on a single process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884124</td>\n",
       "      <td>0.610454</td>\n",
       "      <td>0.600498</td>\n",
       "      <td>0.474669</td>\n",
       "      <td>0.247233</td>\n",
       "      <td>0.357306</td>\n",
       "      <td>0.344136</td>\n",
       "      <td>0.339641</td>\n",
       "      <td>0.600859</td>\n",
       "      <td>0.425705</td>\n",
       "      <td>0.604915</td>\n",
       "      <td>0.419193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.854412</td>\n",
       "      <td>0.604125</td>\n",
       "      <td>0.593634</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.266308</td>\n",
       "      <td>0.261472</td>\n",
       "      <td>0.506387</td>\n",
       "      <td>0.464454</td>\n",
       "      <td>0.665799</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>0.445219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.908983</td>\n",
       "      <td>0.632063</td>\n",
       "      <td>0.557429</td>\n",
       "      <td>0.498264</td>\n",
       "      <td>0.276396</td>\n",
       "      <td>0.312810</td>\n",
       "      <td>0.448530</td>\n",
       "      <td>0.448674</td>\n",
       "      <td>0.649791</td>\n",
       "      <td>0.489869</td>\n",
       "      <td>0.591908</td>\n",
       "      <td>0.450002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7   \\\n",
       "0  0.884124  0.610454  0.600498  0.474669  0.247233  0.357306  0.344136   \n",
       "1  0.854412  0.604125  0.593634  0.495885  0.266308  0.261472  0.506387   \n",
       "2  0.908983  0.632063  0.557429  0.498264  0.276396  0.312810  0.448530   \n",
       "\n",
       "         8         9         10        11        12  \n",
       "0  0.339641  0.600859  0.425705  0.604915  0.419193  \n",
       "1  0.464454  0.665799  0.542969  0.580444  0.445219  \n",
       "2  0.448674  0.649791  0.489869  0.591908  0.450002  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.iloc[:,1:]\n",
    "y = data.iloc[:,0]\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2001\n",
       "1    2001\n",
       "2    2001\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec. 1.1: Separating training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4034, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics, cross_validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1345, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec. 1.2: Linear Regression model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple linear regression, refer to [linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.497287299731 0.476532563406\n"
     ]
    }
   ],
   "source": [
    "## compute R^2 \n",
    "print (lr.score(X_val, y_val), lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def R_squared(predict, true):\n",
    "    if len(predict) != len(true): return False\n",
    "    meanTrue = np.mean(true)\n",
    "    RSS =0\n",
    "    TSS =0 \n",
    "    for i in range(len(predict)):\n",
    "        RSS += (predict[i] - true[i])**2\n",
    "        TSS += (true[i] - meanTrue)**2\n",
    "    return np.sqrt(RSS/len(predict)), 1.0-RSS/TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15.460592450650008, 0.49728729973086772)\n"
     ]
    }
   ],
   "source": [
    "predictedY = lr.predict(X_val)\n",
    "print (R_squared(predictedY, y_val.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec. 1.3: Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (1.3a) Ridge regression (L2)**\n",
    "\n",
    "Refer to the [ridge regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 (15.460592450648022, 0.49728729973099683)\n",
      "1e-05 (15.460592253779264, 0.49728731253366598)\n",
      "0.01 (15.46039861739483, 0.49729990491771214)\n",
      "0.1 (15.458913261095528, 0.49739649400345864)\n",
      "1.0 (15.460306405272094, 0.49730590151660636)\n",
      "2.0 (15.477467888958312, 0.49618926586040824)\n"
     ]
    }
   ],
   "source": [
    "for lambda_i in [1e-10, 1e-5, 1e-2, 0.1, 1.0, 2.0]:\n",
    "    ridge = linear_model.Ridge(alpha=lambda_i)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    predictedY = ridge.predict(X_val)\n",
    "    print (lambda_i, R_squared(predictedY, y_val.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3b) LASSO (L1)\n",
    "\n",
    "Refer to [LASSO regression](http://scikit-learn.org/stable/modules/linear_model.html#lasso) in sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 (15.460592450563135, 0.4972872997365172)\n",
      "1e-05 (15.460583748958394, 0.49728786561470062)\n",
      "0.01 (15.46533750850878, 0.49697867418342156)\n",
      "0.1 (15.778979237144268, 0.47636893652433288)\n",
      "1.0 (20.476642483045577, 0.11816955565414855)\n",
      "2.0 (21.807425432970533, -0.00017589907371395874)\n"
     ]
    }
   ],
   "source": [
    "for lambda_i in [1e-10, 1e-5, 1e-2, 0.1, 1.0, 2.0]:\n",
    "    ridge = linear_model.Lasso(alpha=lambda_i)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    predictedY = ridge.predict(X_val)\n",
    "    print (lambda_i, R_squared(predictedY, y_val.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec. 1.4: Polynomial regression\n",
    "\n",
    "Up to order of 2, i.e. square terms. Refer [the polynomial regression](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "polyreg = Pipeline([('poly', PolynomialFeatures(degree=2)),('linear', LinearRegression(fit_intercept=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linear', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polyreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14.420119802441674, 0.56267396763799549)\n"
     ]
    }
   ],
   "source": [
    "predictedY = polyreg.predict(X_val)\n",
    "print (R_squared(predictedY, y_val.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14.840052019616181, 0.51858814381806306)\n"
     ]
    }
   ],
   "source": [
    "predictedY = polyreg.predict(X_test)\n",
    "print (R_squared(predictedY, y_test.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that considering the order up to second-order polynomials, the mean squared erros descreases and the R-square increases on both validation and test sets. Hence the better model is for second-order polynomial on linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Part 2: Using Spark and MLlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Sec. 2.1: Read and parse the initial dataset **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.1a) Load and check the data **\n",
    "The raw data is currently stored in text file.  We will start by storing this raw data in as an RDD, with each element of the RDD representing a data point as a comma-delimited string. Each string starts with the label (a year) followed by numerical audio features. Use the [count method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) to check how many data points we have.  Then use the [take method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) to create and print out a list of the first 5 data points in their initial string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load testing library\n",
    "from test_helper import Test\n",
    "import os.path\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('cs190', 'millionsong.txt')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 2\n",
    "rawData = sc.textFile(fileName, numPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of observation in the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6724\n"
     ]
    }
   ],
   "source": [
    "numPoints = rawData.count()\n",
    "print numPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primitive data format (reading from the text file) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'2001.0,0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817', u'2001.0,0.854411946129,0.604124786151,0.593634078776,0.495885413963,0.266307830936,0.261472105188,0.506387076327,0.464453565511,0.665798573683,0.542968988766,0.58044428577,0.445219373624']\n"
     ]
    }
   ],
   "source": [
    "samplePoints = rawData.take(2)\n",
    "print samplePoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.1b) Using `LabeledPoint` **\n",
    "In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Write the parsePoint function that takes as input a raw data point, parses it using Python's [unicode.split](https://docs.python.org/2/library/string.html#string.split) method, and returns a `LabeledPoint`.  Use this function to parse samplePoints (from the previous question).  Then print out the features and label for the first training point, using the `LabeledPoint.features` and `LabeledPoint.label` attributes. Finally, calculate the number features for this dataset.\n",
    "Note that `split()` can be called directly on a `unicode` or `str` object.  For example, `u'split,me'.split(',')` returns `[u'split', u'me']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "# Here is a sample raw data point:\n",
    "# '2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419'\n",
    "# In this raw data point, 2001.0 is the label, and the remaining values are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
    "    Args:\n",
    "        line (unicode): Comma separated unicode string where the first element is the label and the\n",
    "            remaining elements are features.\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features.\n",
    "    \"\"\"\n",
    "    parsedLine = line.split(',')\n",
    "    return LabeledPoint(parsedLine[0], parsedLine[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert all raw data to LabeledPoint class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'> <class 'pyspark.rdd.PipelinedRDD'>\n",
      "(2001.0,[0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817])\n",
      "<class 'pyspark.mllib.regression.LabeledPoint'>\n"
     ]
    }
   ],
   "source": [
    "parsedSamplePoints = rawData.map(lambda x: parsePoint(x))\n",
    "print type(rawData), type(parsedSamplePoints)\n",
    "print parsedSamplePoints.first()\n",
    "print type(parsedSamplePoints.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"rawData\" is the RDD class, and \"parsedSamplePoints\" is the pipelinedRDD. Each item of the \"parsedSamplePoints\" is the \"pyspark.mllib.regression.LabeledPoint\" class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.regression.LabeledPoint'>\n",
      "[0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817] 2001.0\n"
     ]
    }
   ],
   "source": [
    "firstPointFeatures = parsedSamplePoints.first().features\n",
    "firstPointLabel = parsedSamplePoints.first().label\n",
    "print firstPointFeatures, firstPointLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "d = len(firstPointFeatures)\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.1c) Class of variables in this note**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print type(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we did: \"parsedSamplePoints = rawData.map(lambda x: parsePoint(x))\" with mapping function on rawData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "print type(parsedSamplePoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "[LabeledPoint(2001.0, [0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817])]\n"
     ]
    }
   ],
   "source": [
    "print type(parsedSamplePoints.take(1))\n",
    "print parsedSamplePoints.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember each item in the \"parsedSamplePoints\" is the \"pyspark.mllib.regression.LabeledPoint\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.regression.LabeledPoint'>\n",
      "(2001.0,[0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817])\n"
     ]
    }
   ],
   "source": [
    "print type(parsedSamplePoints.take(1)[0])\n",
    "print parsedSamplePoints.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.DenseVector'> <type 'float'>\n",
      "[0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817] 2001.0\n"
     ]
    }
   ],
   "source": [
    "print type(parsedSamplePoints.take(1)[0].features), type(parsedSamplePoints.take(1)[0].label)\n",
    "print parsedSamplePoints.take(1)[0].features, parsedSamplePoints.take(1)[0].label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features is represented as DenseVetor class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.1d) Find the range of y (label)**\n",
    "Now let's examine the labels to find the range of song years.  To do this, first parse each element of the `rawData` RDD, and then find the smallest and largest labels. There are two ways to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedDataInit = rawData.map(lambda x: parsePoint(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is that we generate \"paredDataInit\" as pipelinedRDD, and after the collect method, \"onlyLabels\" to become a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "2011.0 1922.0\n"
     ]
    }
   ],
   "source": [
    "onlyLabels = parsedDataInit.map(lambda x: x.label).collect()\n",
    "print type(onlyLabels)\n",
    "minYear = min(onlyLabels)\n",
    "maxYear = max(onlyLabels)\n",
    "print maxYear, minYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is that we deal with \"onlyLabels\" as a pipelinedRDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "2011.0 1922.0\n"
     ]
    }
   ],
   "source": [
    "onlyLabels = parsedDataInit.map(lambda x: x.label)\n",
    "print type(onlyLabels)\n",
    "minYear = onlyLabels.min()\n",
    "maxYear = onlyLabels.max()\n",
    "print maxYear, minYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.1e) Shift labels **\n",
    "As we just saw, the labels are years in the 1900s and 2000s.  In learning problems, it is often natural to shift labels such that they start from zero.  Starting with `parsedDataInit`, create a new RDD consisting of `LabeledPoint` objects in which the labels are shifted such that smallest label equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.mllib.regression.LabeledPoint'>\n",
      "\n",
      "[LabeledPoint(79.0, [0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817])]\n",
      "79.0\n"
     ]
    }
   ],
   "source": [
    "parsedData = parsedDataInit.map(lambda x: LabeledPoint(x.label-minYear, x.features))\n",
    "# Should be a LabeledPoint\n",
    "print type(parsedData)\n",
    "print type(parsedData.take(1)[0])\n",
    "# View the first point\n",
    "print '\\n{0}'.format(parsedData.take(1))\n",
    "print (parsedData.take(1)[0].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After shifting, the maximum and minimum values are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print max(parsedData.map(lambda x: x.label).collect())\n",
    "print min(parsedData.map(lambda x: x.label).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.1f) Training, validation, and test sets **\n",
    "We're almost done parsing our dataset, and our final task involves split it into training, validation and test sets. Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets. Next, cache each of these RDDs, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset and verify that the sum of their sizes equals the value computed in Part (1a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6434] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = parsedData.randomSplit(weights, seed)\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "parsedTestData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5371 682 671 6724\n",
      "6724\n"
     ]
    }
   ],
   "source": [
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "nTest = parsedTestData.count()\n",
    "\n",
    "print nTrain, nVal, nTest, nTrain + nVal + nTest\n",
    "print parsedData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Sec. 2.2: Baseline model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.2a) Average label **\n",
    "A very simple yet natural baseline model is one where we always make the same prediction independent of the given data point, using the average label in the training set as the constant prediction value.  Compute this value, which is the average (shifted) song year for the training set.  Use an appropriate method in the [RDD API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "53.9316700801 53.7287390029 52.6691505216\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "averageTrainYear = parsedTrainData.map(lambda x: x.label).reduce(add)/float(nTrain)\n",
    "averageValYear = parsedValData.map(lambda x: x.label).reduce(add)/float(nVal)\n",
    "averageTestYear = parsedTestData.map(lambda x: x.label).reduce(add)/float(nTest)\n",
    "print type(labelsAndPredsTrain)\n",
    "print averageTrainYear, averageValYear, averageTestYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.2b) Defining root mean squared error **\n",
    "We naturally would like to see how well this naive baseline performs.  We will use root mean squared error ([RMSE](http://en.wikipedia.org/wiki/Root-mean-square_deviation)) for evaluation purposes.  Implement a function to compute RMSE given an RDD of (label, prediction) tuples, and test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    return (float(label) - float(prediction))**2\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    diff = labelsAndPreds.map(lambda x: squaredError(x[0],x[1]))\n",
    "    return np.sqrt(diff.sum()/float(diff.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29099444874\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n",
    "# RMSE = sqrt[((3-1)^2 + (1-2)^2 + (2-2)^2) / 3] = 1.291\n",
    "exampleRMSE = calcRMSE(labelsAndPreds)\n",
    "print exampleRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.2c) Training, validation and test RMSE **\n",
    "Now let's calculate the training, validation and test RMSE of our baseline model. To do this, first create RDDs of (label, prediction) tuples for each dataset, and then call calcRMSE. Note that each RMSE can be interpreted as the average prediction error [we did in (2.2a)] for the given dataset (in terms of number of years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.9316700801 53.7287390029 52.6691505216\n"
     ]
    }
   ],
   "source": [
    "print averageTrainYear, averageValYear, averageTestYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Train RMSE = 21.306\n",
      "Baseline Validation RMSE = 21.585\n",
      "Baseline Test RMSE = 22.101\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTrain = parsedTrainData.map(lambda x: (x.label, averageTrainYear))\n",
    "rmseTrainBase = calcRMSE(labelsAndPredsTrain)\n",
    "# --------------------------\n",
    "labelsAndPredsVal = parsedValData.map(lambda x: (x.label, averageValYear))\n",
    "rmseValBase = calcRMSE(labelsAndPredsVal)\n",
    "# ---------------------------\n",
    "labelsAndPredsTest = parsedTestData.map(lambda x: (x.label, averageTestYear))\n",
    "rmseTestBase = calcRMSE(labelsAndPredsTest)\n",
    "\n",
    "print 'Baseline Train RMSE = {0:.3f}'.format(rmseTrainBase)\n",
    "print 'Baseline Validation RMSE = {0:.3f}'.format(rmseValBase)\n",
    "print 'Baseline Test RMSE = {0:.3f}'.format(rmseTestBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Sec. 2.3: Train (via gradient descent) and evaluate a linear regression model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.3a) Gradient summand **\n",
    "Now let's see if we can do better via linear regression, training a model via gradient descent (we'll omit the intercept for now). Recall that the gradient descent update for linear regression is: \n",
    "$$ \\scriptsize \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha_i \\sum_j (\\mathbf{w}_i^\\top\\mathbf{x}_j  - y_j) \\mathbf{x}_j \\,.$$ \n",
    "where $ \\scriptsize i $ is the iteration number of the gradient descent algorithm, and $ \\scriptsize j $ identifies the observation.\n",
    "\n",
    "First, implement a function that computes the summand for this update, i.e., the summand equals $ \\scriptsize (\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x} \\, ,$ and test out this function on two examples.  Use the `DenseVector` [dot](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector.dot) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientSummand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n",
    "\n",
    "    Returns:\n",
    "        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n",
    "    \"\"\"\n",
    "    return (weights.dot(lp.features)-lp.label) * lp.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.0,6.0,24.0]\n"
     ]
    }
   ],
   "source": [
    "#exampleW = DenseVector([1, 1, 1])\n",
    "exampleW = np.array([1, 1, 1])\n",
    "exampleLP = LabeledPoint(2.0, [3, 1, 4])\n",
    "# gradientSummand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n",
    "summandOne = gradientSummand(exampleW, exampleLP)\n",
    "print summandOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.3b) Use weights to make predictions **\n",
    "Next, implement a `getLabeledPredictions` function that takes in weights and an observation's `LabeledPoint` and returns a (label, prediction) tuple.  Note that we can predict by computing the dot product between weights and an observation's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLabeledPrediction(weights, observation):\n",
    "    \"\"\"Calculates predictions and returns a (label, prediction) tuple.\n",
    "\n",
    "    Note:\n",
    "        The labels should remain unchanged as we'll use this information to calculate prediction\n",
    "        error later.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): An array with one weight for each features in `trainData`.\n",
    "        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n",
    "            features for the data point.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (label, prediction) tuple.\n",
    "    \"\"\"\n",
    "    return (observation.label, DenseVector.dot(DenseVector(weights), observation.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate toy dataset to test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(2.0, [1.0,0.5]), LabeledPoint(1.5, [0.5,0.5])]\n",
      "[(2.0, 1.75), (1.5, 1.25)]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([1.0, 1.5])\n",
    "predictionExample = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n",
    "                                    LabeledPoint(1.5, np.array([.5, .5]))])\n",
    "# w = (1, 1.5)\n",
    "# y1 = 2  , x1 = (1, 0.5)\n",
    "# y2 = 1.5, x2 = (0.5, 0.5)\n",
    "# prediction_1 = w * x1 = (1, 1.5) * (1, 0.5) = 1+ 0.75 = 1.75\n",
    "# prediction_2 = w * x2 = (1, 1.5) * (0.5, 0.5) = 0.5 +0.75 = 1.25\n",
    "print predictionExample.take(2)\n",
    "labelsAndPredsExample = predictionExample.map(lambda lp: getLabeledPrediction(weights, lp))\n",
    "print labelsAndPredsExample.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.3c) Gradient descent **\n",
    "Next, implement a gradient descent function for linear regression and test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linregGradientDescent(trainData, numIters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "    Args:\n",
    "        trainData (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        numIters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = trainData.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(trainData.take(1)[0].features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 1.0\n",
    "    # We will compute and store the training error after each iteration\n",
    "    errorTrain = np.zeros(numIters)\n",
    "    for i in range(numIters):\n",
    "        # Use getLabeledPrediction from (3b) with trainData to obtain an RDD of (label, prediction)\n",
    "        # tuples.  Note that the weights all equal 0 for the first iteration, so the predictions will\n",
    "        # have large errors to start.\n",
    "        labelsAndPredsTrain = trainData.map(lambda lp: getLabeledPrediction(w, lp))\n",
    "        errorTrain[i] = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradientSummand` function you wrote in (3a).\n",
    "        # Note that `gradient` sould be a `DenseVector` of length `d`.\n",
    "        gradient = trainData.map(lambda lp: gradientSummand(w, lp)).reduce(add)\n",
    "        ''' or gradient = trainData.map(lambda lp: gradientSummand(w, lp)).sum() '''\n",
    "        \n",
    "        print i, 'RMSE= ', errorTrain[i]\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "        w -= alpha_i* gradient\n",
    "    return w, errorTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the followings, we create a toy dataset with n = 10 (10 observations), d = 3 (3 features), and then run 5 iterations of gradient descent. Note that the resulting model will not be useful; it is a toy test. The goal here is to check that linregGradientDescent is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(79.0, [0.884123733793,0.610454259079,0.600498416968]), LabeledPoint(79.0, [0.854411946129,0.604124786151,0.593634078776])]\n"
     ]
    }
   ],
   "source": [
    "exampleN = 10\n",
    "exampleD = 3\n",
    "exampleData = (sc.parallelize(parsedTrainData.take(exampleN))\n",
    "               .map(lambda lp: LabeledPoint(lp.label, lp.features[0:exampleD])))\n",
    "print exampleData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting \"exampleWeights\" is 3-dimensional and is a \"np.array\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RMSE=  79.720135474\n",
      "1 RMSE=  30.278356987\n",
      "2 RMSE=  9.27842641274\n",
      "3 RMSE=  9.20967856405\n",
      "4 RMSE=  9.19446482628\n",
      "<type 'numpy.ndarray'> [ 48.88110449  36.01144093  30.25350092]\n"
     ]
    }
   ],
   "source": [
    "exampleNumIters = 5\n",
    "exampleWeights, exampleErrorTrain = linregGradientDescent(exampleData, exampleNumIters)\n",
    "print type(exampleWeights), exampleWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.3d) Train the model **\n",
    "Now let's seriously train a linear regression model on all of our training data and evaluate its accuracy on the validation set.  Note that the test set will not be used here.  If we evaluated the model on the test set, we would bias our final results.\n",
    "\n",
    "We've already done much of the required work: we computed the number of features in Part (2.1b); we created the training and validation datasets and computed their sizes in Part (2.1e); and, we wrote a function to compute RMSE in Part (2.2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RMSE=  57.9876286089\n",
      "1 RMSE=  105.634724083\n",
      "2 RMSE=  111.85702947\n",
      "3 RMSE=  77.8405077938\n",
      "4 RMSE=  39.91899243\n",
      "5 RMSE=  22.7317937659\n",
      "6 RMSE=  20.2054568839\n",
      "7 RMSE=  20.0339150616\n",
      "8 RMSE=  19.9644474223\n",
      "9 RMSE=  19.9011299208\n",
      "10 RMSE=  19.842131161\n",
      "11 RMSE=  19.7868293254\n",
      "12 RMSE=  19.7347370379\n",
      "13 RMSE=  19.6854635199\n",
      "14 RMSE=  19.6386898421\n",
      "15 RMSE=  19.5941517638\n",
      "16 RMSE=  19.5516274995\n",
      "17 RMSE=  19.5109287878\n",
      "18 RMSE=  19.4718942357\n",
      "19 RMSE=  19.4343842674\n",
      "20 RMSE=  19.39827723\n",
      "21 RMSE=  19.3634663481\n",
      "22 RMSE=  19.3298573112\n",
      "23 RMSE=  19.2973663411\n",
      "24 RMSE=  19.2659186269\n",
      "25 RMSE=  19.2354470469\n",
      "26 RMSE=  19.2058911138\n",
      "27 RMSE=  19.1771960996\n",
      "28 RMSE=  19.1493123023\n",
      "29 RMSE=  19.1221944284\n",
      "30 RMSE=  19.0958010694\n",
      "31 RMSE=  19.0700942551\n",
      "32 RMSE=  19.0450390708\n",
      "33 RMSE=  19.0206033275\n",
      "34 RMSE=  18.9967572759\n",
      "35 RMSE=  18.9734733585\n",
      "36 RMSE=  18.9507259921\n",
      "37 RMSE=  18.9284913779\n",
      "38 RMSE=  18.9067473341\n",
      "39 RMSE=  18.885473148\n",
      "40 RMSE=  18.8646494452\n",
      "41 RMSE=  18.844258073\n",
      "42 RMSE=  18.8242819969\n",
      "43 RMSE=  18.8047052076\n",
      "44 RMSE=  18.7855126381\n",
      "45 RMSE=  18.7666900889\n",
      "46 RMSE=  18.7482241606\n",
      "47 RMSE=  18.7301021933\n",
      "48 RMSE=  18.7123122116\n",
      "49 RMSE=  18.6948428746\n"
     ]
    }
   ],
   "source": [
    "numIters = 50\n",
    "weightsLR0, errorTrainLR0 = linregGradientDescent(parsedTrainData,numIters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training via gradient descent is done and we have a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.64535883  20.064699    -0.05341901   8.2931319    5.79155768\n",
      "  -4.51008084  15.23075467   3.8465554    9.91992022   5.97465933\n",
      "  11.36849033   3.86452361]\n"
     ]
    }
   ],
   "source": [
    "print weightsLR0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and calculate RMSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(79.0, 64.012682471741243), (86.0, 48.282734919171631)]\n",
      "Validation RMSE:\n",
      "\tBaseline = 21.585\n",
      "\tLR0 = 19.192\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: getLabeledPrediction(weightsLR0, lp))\n",
    "print labelsAndPreds.take(2)\n",
    "rmseValLR0 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print 'Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(rmseValBase,\n",
    "                                                                       rmseValLR0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.3e) Training error visualization **\n",
    "We will look at the log of the training error as a function of iteration. The plot shows the training error itself, focusing on the final 44 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAIdCAYAAACup5W/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYYQ2AAAAALAwQhsAAAAAWNj/A4xGx/ufo7FWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb090b82c>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm = Normalize()\n",
    "clrs = cmap(np.asarray(norm(errorTrainLR0[6:])))[:,0:3]\n",
    "\n",
    "fig, ax = preparePlot(np.arange(0, 60, 10), np.arange(17, 22, 1))\n",
    "ax.set_ylim(17.8, 21.2)\n",
    "plt.scatter(range(0, numIters-6), errorTrainLR0[6:], s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\n",
    "ax.set_xticklabels(map(str, range(6, 66, 10)))\n",
    "ax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Sec. 2.4: Train using MLlib and perform grid search **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.4a) `LinearRegressionWithSGD` **\n",
    "We're already doing better than the baseline model, but let's see if we can do better by adding an intercept, using regularization, and (based on the previous visualization) training for more iterations.  MLlib's [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD) essentially implements the same algorithm that we implemented in (2.3b), albeit more efficiently and with various additional functionality, such as stochastic gradient approximation, including an intercept in the model and also allowing L1 or L2 regularization.  First use LinearRegressionWithSGD to train a model with L2 regularization and with an intercept.  This method returns a [LinearRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel).  Next, use the model's [weights](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.weights) and [intercept](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.intercept) attributes to print out the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.682292427,14.7439059559,-0.0935105608897,6.22080088829,4.01454261926,-3.30214858535,11.0403027232,2.67190962854,7.18925791279,4.46093254586,8.14950409475,2.75135810882] 13.3335907631\n"
     ]
    }
   ],
   "source": [
    "firstModel = LinearRegressionWithSGD.train(parsedTrainData, step=alpha, \\\n",
    "                                           miniBatchFraction=miniBatchFrac, iterations=numIters,\\\n",
    "                                           regParam=reg, regType=regType, intercept=useIntercept)\n",
    "\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 = firstModel.weights\n",
    "interceptLR1 = firstModel.intercept\n",
    "print weightsLR1, interceptLR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2.4b) Predict**\n",
    "Now use the [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.predict) method to make a prediction on a sample point.  Pass the `features` from a `LabeledPoint` into the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79.0,[0.884123733793,0.610454259079,0.600498416968,0.474669212493,0.247232680947,0.357306088914,0.344136412234,0.339641227335,0.600858840135,0.425704689024,0.60491501652,0.419193351817])\n",
      "56.8013380112\n"
     ]
    }
   ],
   "source": [
    "samplePoint = parsedTrainData.take(1)[0]\n",
    "print samplePoint\n",
    "samplePrediction = firstModel.predict(samplePoint.features)\n",
    "print samplePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.4c) Evaluate RMSE **\n",
    "Next evaluate the accuracy of this model on the validation set.  Use the `predict()` method to create a `labelsAndPreds` RDD, and then use the `calcRMSE()` function from (2.2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 21.585\n",
      "\tLR0 = 19.192\n",
      "\tLR1 = 19.691\n"
     ]
    }
   ],
   "source": [
    "labelsAndPreds = parsedValData.map(lambda lp: (lp.label, firstModel.predict(lp.features)))\n",
    "rmseValLR1 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}' +\n",
    "       '\\n\\tLR1 = {2:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.4d) Grid search **\n",
    "We're already outperforming the baseline on the validation set by almost 2 years on average, but let's see if we can do better. Perform grid search to find a good regularization parameter.  Try `regParam` values `1e-10`, `1e-5`, and `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-10 17.0171700716\n",
      "1e-05 17.0175981807\n",
      "1 23.8007746698\n"
     ]
    }
   ],
   "source": [
    "bestRMSE = rmseValLR1\n",
    "bestRegParam = reg\n",
    "bestModel = firstModel\n",
    "\n",
    "numIters = 500\n",
    "alpha = 1.0\n",
    "miniBatchFrac = 1.0\n",
    "for reg in [1e-10, 1e-5, 1]:\n",
    "    model = LinearRegressionWithSGD.train(parsedTrainData, numIters, alpha,\n",
    "                                          miniBatchFrac, regParam=reg,\n",
    "                                          regType='l2', intercept=True)\n",
    "    labelsAndPreds = parsedValData.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "    rmseValGrid = calcRMSE(labelsAndPreds)\n",
    "    print reg, rmseValGrid\n",
    "    if rmseValGrid < bestRMSE:\n",
    "        bestRMSE = rmseValGrid\n",
    "        bestRegParam = reg\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 21.585\n",
      "\tLR0 = 19.192\n",
      "\tLR1 = 19.691\n",
      "\tLRGrid = 17.017\n"
     ]
    }
   ],
   "source": [
    "rmseValLRGrid = bestRMSE\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n' +\n",
    "       '\\tLRGrid = {3:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1, rmseValLRGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.4e) Vary alpha (learning rate) and the number of iterations **\n",
    "In the previous grid search, we set `alpha = 1` for all experiments.  Now let's see what happens when we vary `alpha`.  Specifically, try `1e-5` and `10` as values for `alpha` and also try training models for 500 iterations (as before) but also for 5 iterations. Evaluate all models on the validation set.  Note that if we set `alpha` too small the gradient descent will require a huge number of steps to converge to the solution, and if we use too large of an `alpha` it can cause numerical problems, like you'll see below for `alpha = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1e-05, numIters = 5, RMSE = 56.970\n",
      "alpha = 1e-05, numIters = 500, RMSE = 56.893\n",
      "alpha = 1e+01, numIters = 5, RMSE = 355124752.221\n",
      "alpha = 1e+01, numIters = 500, RMSE = 33110728225678989137251839534941311488306376280786874812632607716020409015644103457295574688229365257796099669135380709376.000\n"
     ]
    }
   ],
   "source": [
    "reg = bestRegParam\n",
    "modelRMSEs = []\n",
    "for alpha in [1e-5, 10]:\n",
    "    for numIters in [5,500]:\n",
    "        model = LinearRegressionWithSGD.train(parsedTrainData, numIters, alpha,\n",
    "                                              miniBatchFrac, regParam=reg,\n",
    "                                              regType='l2', intercept=True)\n",
    "        labelsAndPreds = parsedValData.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "        rmseVal = calcRMSE(labelsAndPreds)\n",
    "        print 'alpha = {0:.0e}, numIters = {1}, RMSE = {2:.3f}'.format(alpha, numIters, rmseVal)\n",
    "        modelRMSEs.append(rmseVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Sec. 2.5: Add interactions between features **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.5a) Add 2-way interactions **\n",
    "So far, we've used the features as they were provided.  Now, we will add features that capture the two-way interactions between our existing features.  Write a function `twoWayInteractions` that takes in a `LabeledPoint` and generates a new `LabeledPoint` that contains the old features and the two-way interactions between them.  Note that a dataset with three features would have nine ( $ \\scriptsize 3^2 $ ) two-way interactions.\n",
    "\n",
    "You might want to use [itertools.product](https://docs.python.org/2/library/itertools.html#itertools.product) to generate tuples for each of the possible 2-way interactions.  Remember that you can combine two `DenseVector` or `ndarray` objects using [np.hstack](http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html#numpy.hstack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def twoWayInteractions(lp):\n",
    "    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n",
    "\n",
    "    Note:\n",
    "        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n",
    "        would be appended to the original [x, y] feature list.\n",
    "\n",
    "    Args:\n",
    "        lp (LabeledPoint): The label and features for this observation.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n",
    "            should include the features from `lp` followed by the two-way interaction features.\n",
    "    \"\"\"\n",
    "    twoTerms = [x*y for x in lp.features for y in lp.features]\n",
    "    return LabeledPoint(lp.label, np.hstack((lp.features, twoTerms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[2.0,3.0,4.0,6.0,6.0,9.0])\n"
     ]
    }
   ],
   "source": [
    "print twoWayInteractions(LabeledPoint(0.0, [2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the existing train, validation, and test sets to include two-way interactions.\n",
    "trainDataInteract = parsedTrainData.map(lambda lp: twoWayInteractions(lp))\n",
    "valDataInteract = parsedValData.map(lambda lp: twoWayInteractions(lp))\n",
    "testDataInteract = parsedTestData.map(lambda lp: twoWayInteractions(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print len(parsedTrainData.take(1)[0].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of features is 12+12**2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "print len(trainDataInteract.take(1)[0].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.5b) Build interaction model **\n",
    "Now, let's build the new model.  We've done this several times now.  To implement this for the new features, we need to change a few variable names.  Remember that we should build our model from the training data and evaluate it on the validation data.\n",
    "\n",
    " Note that you should re-run your hyperparameter search after changing features, as using the best hyperparameters from your prior model will not necessary lead to the best model.  For this exercise, we have already preset the hyperparameters to reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 21.585\n",
      "\tLR0 = 19.192\n",
      "\tLR1 = 19.691\n",
      "\tLRGrid = 17.017\n",
      "\tLRInteract = 15.690\n"
     ]
    }
   ],
   "source": [
    "numIters = 500\n",
    "alpha = 1.0\n",
    "miniBatchFrac = 1.0\n",
    "reg = 1e-10\n",
    "\n",
    "modelInteract = LinearRegressionWithSGD.train(trainDataInteract, numIters, alpha,\n",
    "                                              miniBatchFrac, regParam=reg,\n",
    "                                              regType='l2', intercept=True)\n",
    "labelsAndPredsInteract = valDataInteract.map(lambda lp: (lp.label, modelInteract.predict(lp.features)))\n",
    "rmseValInteract = calcRMSE(labelsAndPredsInteract)\n",
    "\n",
    "print ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' +\n",
    "       '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(rmseValBase, rmseValLR0, rmseValLR1,\n",
    "                                                 rmseValLRGrid, rmseValInteract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** (2.5c) Evaluate interaction model on test data **\n",
    "Our final step is to evaluate the new model on the test dataset.  Note that we haven't used the test set to evaluate any of our models.  Because of this, our evaluation provides us with an unbiased estimate for how our model will perform on new data.  If we had changed our model based on viewing its performance on the test set, our estimate of RMSE would likely be overly optimistic.\n",
    "\n",
    "We'll also print the RMSE for both the baseline model and our new model.  With this information, we can see how much better our model performs than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE:\n",
      "\tBaseline = 22.101\n",
      "\tLRInteract = 16.327\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTest = testDataInteract.map(lambda lp: (lp.label, modelInteract.predict(lp.features)))\n",
    "rmseTestInteract = calcRMSE(labelsAndPredsTest)\n",
    "print ('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'\n",
    "       .format(rmseTestBase, rmseTestInteract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see with considering the second-order terms, the RMSE's are 15.690 and 16.327 for validation and test sets, respectively. Without the terms the RMSE's are 17.017 and 17.344 for validation and test sets. Hence, similar to the case using sklearn, the second-order polynomial linear model improves the prediciton performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
